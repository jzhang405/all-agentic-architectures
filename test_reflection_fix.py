#!/usr/bin/env python3
"""æµ‹è¯•åæ€æ¶æ„ä¿®å¤åçš„å·¥ä½œæµ"""

import os
import sys
import json
from typing import List, TypedDict, Optional
from dotenv import load_dotenv

# OpenAIå…¼å®¹æ¥å£å’ŒLangChainç»„ä»¶
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END

# ç”¨äºç¾è§‚çš„æ‰“å°è¾“å‡º
from rich.console import Console
from rich.markdown import Markdown
from rich.syntax import Syntax

# --- APIå¯†é’¥è®¾ç½® ---
load_dotenv()

class ReflectionState(TypedDict):
    """è¡¨ç¤ºæˆ‘ä»¬åæ€å›¾çš„çŠ¶æ€ã€‚"""
    user_request: str
    draft: Optional[dict]
    critique: Optional[dict]
    refined_code: Optional[dict]

def generator_node(state):
    """ç”Ÿæˆä»£ç çš„åˆå§‹è‰ç¨¿ã€‚"""
    print("--- 1. ç”Ÿæˆåˆå§‹è‰ç¨¿ ---")
    # ä½¿ç”¨å…¼å®¹OpenAI APIçš„JSON schemaæ ¼å¼
    generator_llm = llm.with_structured_output({
        "type": "json_schema",
        "json_schema": {
            "name": "draft_code",
            "schema": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "ç”Ÿæˆçš„ç”¨äºè§£å†³ç”¨æˆ·è¯·æ±‚çš„Pythonä»£ç ã€‚"},
                    "explanation": {"type": "string", "description": "ä»£ç å·¥ä½œåŸç†çš„ç®€è¦è¯´æ˜ã€‚"}
                },
                "required": ["code", "explanation"],
                "additionalProperties": False
            }
        }
    })

    prompt = f"""ä½ æ˜¯ä¸€ä½Pythonä¸“å®¶ç¨‹åºå‘˜ã€‚ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è§£å†³ä»¥ä¸‹è¯·æ±‚ã€‚
    æä¾›ä¸€ä¸ªç®€å•ã€æ¸…æ™°çš„å®ç°å’Œè¯´æ˜ã€‚

    è¯·æ±‚ï¼š{state['user_request']}

    è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«codeå’Œexplanationä¸¤ä¸ªå­—æ®µã€‚"""

    draft = generator_llm.invoke(prompt)
    return {"draft": draft.content}

def critic_node(state):
    """æ‰¹åˆ¤ç”Ÿæˆçš„ä»£ç æ˜¯å¦æœ‰é”™è¯¯å’Œæ•ˆç‡é—®é¢˜ã€‚"""
    print("--- 2. æ‰¹åˆ¤è‰ç¨¿ ---")
    # ä½¿ç”¨å…¼å®¹OpenAI APIçš„JSON schemaæ ¼å¼
    critic_llm = llm.with_structured_output({
        "type": "json_schema",
        "json_schema": {
            "name": "critique",
            "schema": {
                "type": "object",
                "properties": {
                    "has_errors": {"type": "boolean", "description": "ä»£ç æ˜¯å¦æœ‰æ½œåœ¨çš„é”™è¯¯æˆ–é€»è¾‘é”™è¯¯ï¼Ÿ"},
                    "is_efficient": {"type": "boolean", "description": "ä»£ç æ˜¯å¦ä»¥é«˜æ•ˆå’Œæœ€ä¼˜çš„æ–¹å¼ç¼–å†™ï¼Ÿ"},
                    "suggested_improvements": {"type": "array", "items": {"type": "string"}, "description": "æ”¹è¿›ä»£ç çš„å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ã€‚"},
                    "critique_summary": {"type": "string", "description": "æ‰¹åˆ¤çš„æ‘˜è¦ã€‚"}
                },
                "required": ["has_errors", "is_efficient", "suggested_improvements", "critique_summary"],
                "additionalProperties": False
            }
        }
    })

    code_to_critique = state['draft']['code']

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä»£ç å®¡æŸ¥å‘˜å’Œé«˜çº§Pythonå¼€å‘äººå‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹ä»¥ä¸‹ä»£ç è¿›è¡Œå½»åº•çš„æ‰¹åˆ¤ã€‚

    åˆ†æä»£ç çš„ä»¥ä¸‹æ–¹é¢ï¼š
    1.  **é”™è¯¯å’Œç¼ºé™·ï¼š** æ˜¯å¦æœ‰ä»»ä½•æ½œåœ¨çš„è¿è¡Œæ—¶é”™è¯¯ã€é€»è¾‘ç¼ºé™·æˆ–æœªå¤„ç†çš„è¾¹ç¼˜æƒ…å†µï¼Ÿ
    2.  **æ•ˆç‡å’Œæœ€ä½³å®è·µï¼š** è¿™æ˜¯è§£å†³é—®é¢˜çš„æœ€æœ‰æ•ˆæ–¹å¼å—ï¼Ÿå®ƒæ˜¯å¦éµå¾ªæ ‡å‡†çš„Pythonçº¦å®šï¼ˆPEP 8ï¼‰ï¼Ÿ

    æä¾›ç»“æ„åŒ–çš„æ‰¹åˆ¤å’Œå…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ã€‚

    è¦å®¡æŸ¥çš„ä»£ç ï¼š
    ```python
    {code_to_critique}
    ```

    è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«has_errorsã€is_efficientã€suggested_improvementså’Œcritique_summaryå­—æ®µã€‚"""

    critique = critic_llm.invoke(prompt)
    return {"critique": critique.content}

def refiner_node(state):
    """åŸºäºæ‰¹åˆ¤å®Œå–„ä»£ç ã€‚"""
    print("--- 3. å®Œå–„ä»£ç  ---")
    # ä½¿ç”¨å…¼å®¹OpenAI APIçš„JSON schemaæ ¼å¼
    refiner_llm = llm.with_structured_output({
        "type": "json_schema",
        "json_schema": {
            "name": "refined_code",
            "schema": {
                "type": "object",
                "properties": {
                    "refined_code": {"type": "string", "description": "æœ€ç»ˆæ”¹è¿›çš„Pythonä»£ç ã€‚"},
                    "refinement_summary": {"type": "string", "description": "åŸºäºæ‰¹åˆ¤æ‰€åšæ›´æ”¹çš„æ‘˜è¦ã€‚"}
                },
                "required": ["refined_code", "refinement_summary"],
                "additionalProperties": False
            }
        }
    })

    draft_code = state['draft']['code']
    critique_suggestions = json.dumps(state['critique'], indent=2)

    prompt = f"""ä½ æ˜¯ä¸€ä½Pythonä¸“å®¶ç¨‹åºå‘˜ï¼Œè´Ÿè´£åŸºäºæ‰¹åˆ¤å®Œå–„ä¸€æ®µä»£ç ã€‚

    ä½ çš„ç›®æ ‡æ˜¯é‡å†™åŸå§‹ä»£ç ï¼Œå®ç°æ‰¹åˆ¤ä¸­çš„æ‰€æœ‰å»ºè®®æ”¹è¿›ã€‚

    **åŸå§‹ä»£ç ï¼š**
    ```python
    {draft_code}
    ```

    **æ‰¹åˆ¤å’Œå»ºè®®ï¼š**
    {critique_suggestions}

    è¯·æä¾›æœ€ç»ˆå®Œå–„çš„ä»£ç å’Œä½ æ‰€åšæ›´æ”¹çš„æ‘˜è¦ã€‚

    è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«refined_codeå’Œrefinement_summaryå­—æ®µã€‚"""

    refined_code = refiner_llm.invoke(prompt)
    return {"refined_code": refined_code.content}

def main():
    """æµ‹è¯•åæ€æ¶æ„å·¥ä½œæµ"""

    # æ£€æŸ¥APIå¯†é’¥
    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°OPENAI_API_KEYã€‚è¯·æ£€æŸ¥.envæ–‡ä»¶ã€‚")
        sys.exit(1)

    # åˆå§‹åŒ–LLM
    global llm
    llm = ChatOpenAI(
        model=os.environ.get("OPENAI_MODEL", "deepseek-chat"),
        base_url=os.environ.get("OPENAI_BASE_URL", "https://api.deepseek.com/v1"),
        api_key=os.environ.get("OPENAI_API_KEY"),
        temperature=0.2
    )

    print("âœ… LLMåˆå§‹åŒ–æˆåŠŸ")

    # æ„å»ºå›¾
    graph_builder = StateGraph(ReflectionState)
    graph_builder.add_node("generator", generator_node)
    graph_builder.add_node("critic", critic_node)
    graph_builder.add_node("refiner", refiner_node)
    graph_builder.set_entry_point("generator")
    graph_builder.add_edge("generator", "critic")
    graph_builder.add_edge("critic", "refiner")
    graph_builder.add_edge("refiner", END)

    reflection_app = graph_builder.compile()

    print("âœ… åæ€å›¾ç¼–è¯‘æˆåŠŸ")

    # æµ‹è¯•è¯·æ±‚
    user_request = "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥æŸ¥æ‰¾ç¬¬nä¸ªæ–æ³¢é‚£å¥‘æ•°ã€‚"
    initial_input = {"user_request": user_request}

    print(f"\nğŸš€ å¯åŠ¨åæ€å·¥ä½œæµï¼Œè¯·æ±‚ï¼š'{user_request}'\n")

    try:
        # è¿è¡Œå·¥ä½œæµ
        final_state = None
        for state_update in reflection_app.stream(initial_input, stream_mode="values"):
            final_state = state_update

        print("\nâœ… åæ€å·¥ä½œæµå®Œæˆï¼")

        # æ£€æŸ¥ç»“æœ
        if final_state and 'draft' in final_state and 'critique' in final_state and 'refined_code' in final_state:
            print("\n--- åˆå§‹è‰ç¨¿ ---")
            print(f"è¯´æ˜ï¼š{final_state['draft']['explanation']}")
            print(f"ä»£ç ï¼š\n{final_state['draft']['code']}")

            print("\n--- æ‰¹åˆ¤ ---")
            print(f"æ‘˜è¦ï¼š{final_state['critique']['critique_summary']}")
            print("å»ºè®®æ”¹è¿›ï¼š")
            for improvement in final_state['critique']['suggested_improvements']:
                print(f"  - {improvement}")

            print("\n--- æœ€ç»ˆå®Œå–„ä»£ç  ---")
            print(f"å®Œå–„æ‘˜è¦ï¼š{final_state['refined_code']['refinement_summary']}")
            print(f"ä»£ç ï¼š\n{final_state['refined_code']['refined_code']}")

            print("\nğŸ‰ æµ‹è¯•æˆåŠŸï¼åæ€æ¶æ„å·¥ä½œæµè¿è¡Œæ­£å¸¸ã€‚")
            return True
        else:
            print("âŒ é”™è¯¯ï¼šfinal_stateä¸å®Œæ•´")
            return False

    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)